# Создание языковой модели на основе трансформерной архитектуры для абхазского языка

**Выполнил:**
Барателиа Мирон Бесланович

**Проверил:**
Игнатов Дмитрий Игоревич

## О проекте

В рамках данного проекта созданы инструменты для работы с абхазским языком, являющимся малоресурсным. Реализована модель машинного перевода на основе архитектуры MarianMT (трансформер) для пары абхазский-русский, а также разработана языковая модель (LLM), способная понимать и генерировать текст на абхазском языке. Основной сложностью является наращивание текстовых данных и адаптация существующих подходов с целью достижения высокого качества перевода и генерации текста.

Результаты работы будут доступны на [apsua.tech](https://apsua.tech).

## Структура репозитория

*   **`report.pdf`**: Итоговый отчет по исследовательскому проекту, содержащий детальное описание методологии, экспериментов и результатов.
*   **`presentation.pdf`**: Презентация, обобщающая основные этапы и выводы проекта.
*   **`translator/`**: Директория с финальной реализацией кода для обучения модели-переводчика.
    *   `main.py`: Главный скрипт для запуска процесса обучения.
    *   `train.py`: Модуль, содержащий логику цикла обучения.
    *   `model.py`: Модуль для инициализации модели MarianMT.
    *   `data_utils.py`: Модуль для загрузки, обработки и аугментации данных.
    *   `callbacks.py`: Модуль с пользовательскими коллбэками для Hugging Face Trainer.

Конфигурация обучения (гиперпараметры, пути к данным и т.д.) задается в словаре `first_run_config` внутри `main.py`. Логирование экспериментов осуществляется с помощью Weights & Biases (`wandb`). 